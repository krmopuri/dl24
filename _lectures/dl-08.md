---
type: lecture
date: 2024-01-24
title: (dl-08) Training DNNs - I

# optional
# please use /static_files/notes directory to store notes
# thumbnail: /static_files/path/to/image.jpg

# optional
tldr: "Issues with Gradient Descent, specifically with the learning rate "
  
# optional
# set it to true if you dont want this lecture to appear in the updates section
hide_from_announcments: false

# optional
links: 
    #- url: /static_files/presentations/lec.zip
    #  name: notes
    - url: https://colab.research.google.com/drive/1L5yEFMevk9EYiWa1V4p7-uTDA7cQEmCn?usp=sharing
      name: codes - Optimizing 1D Convex function
    - url: https://colab.research.google.com/drive/1eMBFrC9ms-qQVVUJOGJL1Pm28ehKKI4N?usp=sharing
      name: codes - Optimizing 2D Quadratic function
    - url: /static_files/presentations/dl-08.pdf
      name: slides
    #- url: /static_files/presentations/lec.zip
    #  name: other
---

**Suggested Readings:**

- [Ruder's blog on variants of gradient-based updates](https://www.ruder.io/optimizing-gradient-descent/)

